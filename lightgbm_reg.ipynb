{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lightgbm-reg",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPXZcOlwjjEdvBm2Ny6BnY/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shin04/ion-switching/blob/master/lightgbm_reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJNHw6jrSCvs",
        "colab_type": "code",
        "outputId": "4a167b50-8cba-4dc5-a553-28d52961ccfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.4.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRVk22KlSPu0",
        "colab_type": "code",
        "outputId": "bb93cb5a-fba7-4560-d0fb-ec085f930b43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename = \"/root/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7oEMFs7SR7M",
        "colab_type": "code",
        "outputId": "67c226d0-fc18-4bad-899e-8df6f36eb0ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!kaggle datasets download -d cdeotte/data-without-drift"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data-without-drift.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqLv3DkuSkXf",
        "colab_type": "code",
        "outputId": "62175fae-47b8-451b-8e6b-f6c62874e7d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!kaggle datasets download -d sggpls/ion-shifted-rfc-proba"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ion-shifted-rfc-proba.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA9CDVT3tAIm",
        "colab_type": "code",
        "outputId": "3362316e-c4e1-4da0-be65-3e6f26c726b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!kaggle datasets download -d ragnar123/clean-kalman"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean-kalman.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpUupDkYSl3M",
        "colab_type": "code",
        "outputId": "b0a408e1-4dc9-48e1-ad33-6102432f9904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!kaggle competitions download -c liverpool-ion-switching"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79QyU1isSnZH",
        "colab_type": "code",
        "outputId": "cdc1dd32-a791-4706-cd19-94e06d884186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!unzip -o '*.zip'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  clean-kalman.zip\n",
            "  inflating: test_clean_kalman.csv   \n",
            "  inflating: train_clean_kalman.csv  \n",
            "\n",
            "Archive:  ion-shifted-rfc-proba.zip\n",
            "  inflating: Y_test_proba.npy        \n",
            "  inflating: Y_train_proba.npy       \n",
            "\n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n",
            "\n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n",
            "\n",
            "Archive:  data-without-drift.zip\n",
            "  inflating: test_clean.csv          \n",
            "  inflating: train_clean.csv         \n",
            "\n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "\n",
            "6 archives were successfully processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wveF_VMWw8PG",
        "colab_type": "code",
        "outputId": "35162f6a-8b78-4f6a-9bc8-b3f6f8d0edb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install tensorflow_addons"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLiQ1cLhSpr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv1D, Input, Dense, Add, Multiply, BatchNormalization, Activation, Dropout\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import losses, models, optimizers\n",
        "import tensorflow_addons as tfa\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from scipy import signal\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 1000)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDn2jHk8Svhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# configurations and main hyperparammeters\n",
        "EPOCHS = 110\n",
        "NNBATCHSIZE = 16\n",
        "GROUP_BATCH_SIZE = 4000\n",
        "SEED = 321\n",
        "LR = 0.001\n",
        "SPLITS = 5\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd5ZOykHSyC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read data\n",
        "def read_data():\n",
        "\n",
        "    train = pd.read_csv('./train_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n",
        "    test  = pd.read_csv('./test_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32})\n",
        "    sub  = pd.read_csv('./sample_submission.csv', dtype={'time': np.float32})\n",
        "    \n",
        "    Y_train_proba = np.load(\"./Y_train_proba.npy\")\n",
        "    Y_test_proba = np.load(\"./Y_test_proba.npy\")\n",
        "    \n",
        "    for i in range(11):\n",
        "        train[f\"proba_{i}\"] = Y_train_proba[:, i]\n",
        "        test[f\"proba_{i}\"] = Y_test_proba[:, i]\n",
        "\n",
        "    return train, test, sub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZTrfQXOLwvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batching_10(train, test):\n",
        "    # concatenate data\n",
        "    # batchを1から10までフル　ややこしい\n",
        "    batch = 50\n",
        "    total_batches = 14\n",
        "    train['set'] = 'train'\n",
        "    test['set'] = 'test'\n",
        "    data = pd.concat([train, test])\n",
        "    for i in range(int(total_batches)):\n",
        "        data.loc[(data['time'] > i * batch) & (data['time'] <= (i + 1) * batch), 'batch'] = i + 1\n",
        "    train = data[data['set'] == 'train']\n",
        "    test = data[data['set'] == 'test']\n",
        "    train.drop(['set'], inplace = True, axis = 1)\n",
        "    test.drop(['set'], inplace = True, axis = 1)\n",
        "    del data\n",
        "    return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3nWKBrr3LKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_signal_mod(train):\n",
        "    left = 3641000\n",
        "    right = 3829000\n",
        "    thresh_dict = {\n",
        "        3: [0.1, 2.0],\n",
        "        2: [-1.1, 0.7],\n",
        "        1: [-2.3, -0.6],\n",
        "        0: [-3.8, -2],\n",
        "    }\n",
        "    \n",
        "    # train['signal_mod'] = train['signal'].values\n",
        "    for ch in train[train['batch']==8]['open_channels'].unique():\n",
        "        idxs_noisy = (train['open_channels']==ch) & (left<train.index) & (train.index<right)\n",
        "        idxs_not_noisy = (train['open_channels']==ch) & ~idxs_noisy\n",
        "        mean = train[idxs_not_noisy]['signal'].mean()\n",
        "\n",
        "        idxs_outlier = idxs_noisy & (thresh_dict[ch][1]<train['signal'].values)\n",
        "        train['signal'][idxs_outlier]  = mean\n",
        "        idxs_outlier = idxs_noisy & (train['signal'].values<thresh_dict[ch][0])\n",
        "        train['signal'][idxs_outlier]  = mean\n",
        "    return train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81-K4Yfki71w",
        "colab_type": "code",
        "outputId": "f917894a-e383-414c-8880-53a72caae7fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# !pip install pykalman"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pykalman in /usr/local/lib/python3.6/dist-packages (0.9.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QWS9pipiY3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pykalman import KalmanFilter\n",
        "\n",
        "# def Kalman1D(observations, damping=1):\n",
        "#     # To return the smoothed time series data\n",
        "#     observation_covariance = damping\n",
        "#     initial_value_guess = observations[0]\n",
        "#     transition_matrix = 1\n",
        "#     transition_covariance = 0.1\n",
        "#     initial_value_guess\n",
        "#     kf = KalmanFilter(\n",
        "#             initial_state_mean=initial_value_guess,\n",
        "#             initial_state_covariance=observation_covariance,\n",
        "#             observation_covariance=observation_covariance,\n",
        "#             transition_covariance=transition_covariance,\n",
        "#             transition_matrices=transition_matrix\n",
        "#         )\n",
        "#     pred_state, state_cov = kf.smooth(observations)\n",
        "#     return pred_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_pkElMAS8fA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create batches of 4000 observations\n",
        "def batching(df, batch_size):\n",
        "    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n",
        "    df['group'] = df['group'].astype(np.uint16)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzokhYjyS-dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize the data (standard scaler). We can also try other scalers for a better score!\n",
        "def normalize(train, test):\n",
        "    train_input_mean = train.signal.mean()\n",
        "    train_input_sigma = train.signal.std()\n",
        "    train['signal'] = (train.signal - train_input_mean) / train_input_sigma\n",
        "    test['signal'] = (test.signal - train_input_mean) / train_input_sigma\n",
        "    \n",
        "    return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qez_-uQBt-9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_category(train, test):\n",
        "  train[\"category\"] = 0\n",
        "  test[\"category\"] = 0\n",
        "\n",
        "  # train segments with more then 9 open channels classes\n",
        "  train.loc[2000000:2500000-1, 'category'] = 1\n",
        "  train.loc[4500000:5000000-1, 'category'] = 1\n",
        "\n",
        "  # test segments with more then 9 open channels classes (potentially)\n",
        "  test.loc[500000:600000-1, \"category\"] = 1\n",
        "  test.loc[700000:800000-1, \"category\"] = 1\n",
        "  \n",
        "  return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi8ajG9drc0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# signal processing features\n",
        "def calc_gradients(s, n_grads = 4):\n",
        "    '''\n",
        "    Calculate gradients for a pandas series. Returns the same number of samples\n",
        "    '''\n",
        "    grads = pd.DataFrame()\n",
        "    \n",
        "    g = s.values\n",
        "    for i in range(n_grads):\n",
        "        g = np.gradient(g)\n",
        "        grads['grad_' + str(i+1)] = g\n",
        "        \n",
        "    return grads\n",
        "\n",
        "def calc_low_pass(s, n_filts=10):\n",
        "    '''\n",
        "    Applies low pass filters to the signal. Left delayed and no delayed\n",
        "    '''\n",
        "    wns = np.logspace(-2, -0.3, n_filts)\n",
        "    \n",
        "    low_pass = pd.DataFrame()\n",
        "    x = s.values\n",
        "    for wn in wns:\n",
        "        b, a = signal.butter(1, Wn=wn, btype='low')\n",
        "        zi = signal.lfilter_zi(b, a)\n",
        "        low_pass['lowpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n",
        "        low_pass['lowpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n",
        "        \n",
        "    return low_pass\n",
        "\n",
        "def calc_high_pass(s, n_filts=10):\n",
        "    '''\n",
        "    Applies high pass filters to the signal. Left delayed and no delayed\n",
        "    '''\n",
        "    wns = np.logspace(-2, -0.1, n_filts)\n",
        "    \n",
        "    high_pass = pd.DataFrame()\n",
        "    x = s.values\n",
        "    for wn in wns:\n",
        "        b, a = signal.butter(1, Wn=wn, btype='high')\n",
        "        zi = signal.lfilter_zi(b, a)\n",
        "        high_pass['highpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n",
        "        high_pass['highpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n",
        "        \n",
        "    return high_pass\n",
        "\n",
        "def calc_ewm(s, windows=[10, 50, 100, 500, 1000]):\n",
        "    '''\n",
        "    Calculates exponential weighted functions\n",
        "    '''\n",
        "    ewm = pd.DataFrame()\n",
        "    for w in windows:\n",
        "        ewm['ewm_mean_' + str(w)] = s.ewm(span=w, min_periods=1).mean()\n",
        "        ewm['ewm_std_' + str(w)] = s.ewm(span=w, min_periods=1).std()\n",
        "        \n",
        "    # add zeros when na values (std)\n",
        "    ewm = ewm.fillna(value=0)\n",
        "        \n",
        "    return ewm\n",
        "\n",
        "\n",
        "def add_features(s):\n",
        "    '''\n",
        "    All calculations together\n",
        "    '''\n",
        "    \n",
        "    gradients = calc_gradients(s)\n",
        "    low_pass = calc_low_pass(s)\n",
        "    high_pass = calc_high_pass(s)\n",
        "    ewm = calc_ewm(s)\n",
        "    \n",
        "    return pd.concat([s, gradients, low_pass, high_pass, ewm], axis=1)\n",
        "\n",
        "\n",
        "# signal_size を考える\n",
        "# 500000? or 4000?\n",
        "def divide_and_add_features(s, signal_size=500000):\n",
        "    '''\n",
        "    Divide the signal in bags of \"signal_size\".\n",
        "    Normalize the data dividing it by 15.0\n",
        "    '''\n",
        "    # normalize\n",
        "    s = s / 15.0\n",
        "    \n",
        "    ls = []\n",
        "    for i in tqdm(range(int(s.shape[0]/signal_size))):\n",
        "        sig = s[i*signal_size:(i+1)*signal_size].copy().reset_index(drop=True)\n",
        "        sig_featured = add_features(sig)\n",
        "        ls.append(sig_featured)\n",
        "    \n",
        "    return pd.concat(ls, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvqlr0beS_62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get lead and lags features\n",
        "def lag_with_pct_change(df, windows):\n",
        "    for window in windows:    \n",
        "        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n",
        "        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWSG9xWHWlhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_roll_stats(df, windows, group='group'):\n",
        "    '''\n",
        "    Calculates rolling stats like mean, std, min, max...\n",
        "    '''\n",
        "    for i, window in enumerate(windows):\n",
        "      df[group + 'roll_mean_' + str(window)] = df.groupby(group)['signal'].rolling(window=window, min_periods=window).mean().values\n",
        "      df[group + 'roll_std_' + str(window)] = df.groupby(group)['signal'].rolling(window=window, min_periods=window).std().values\n",
        "      df[group + 'roll_min_' + str(window)] = df.groupby(group)['signal'].rolling(window=window, min_periods=window).min().values\n",
        "      df[group + 'roll_max_' + str(window)] = df.groupby(group)['signal'].rolling(window=window, min_periods=window).max().values\n",
        "      df[group + 'roll_range' + str(window)] = df[group + 'roll_max_' + str(window)] - df[group + 'roll_min_' + str(window)]\n",
        "\n",
        "      df['roll_q10_' + str(window)] = df.groupby('group')['signal'].rolling(window=window, min_periods=window).quantile(0.10).values\n",
        "      df['roll_q25_' + str(window)] = df.groupby('group')['signal'].rolling(window=window, min_periods=window).quantile(0.25).values\n",
        "      df['roll_q50_' + str(window)] = df.groupby('group')['signal'].rolling(window=window, min_periods=window).quantile(0.50).values\n",
        "      df['roll_q75_' + str(window)] = df.groupby('group')['signal'].rolling(window=window, min_periods=window).quantile(0.75).values\n",
        "      df['roll_q90_' + str(window)] = df.groupby('group')['signal'].rolling(window=window, min_periods=window).quantile(0.90).values\n",
        "             \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sJgvILz6ppL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_expand_stats(df, group='group'):\n",
        "  df['expanding_mean'] = df.groupby(group)['signal'].expanding().mean().fillna(0).values\n",
        "  df['expanding_std'] = df.groupby(group)['signal'].expanding().std().fillna(0).values\n",
        "  df['expanding_max'] = df.groupby(group)['signal'].expanding().max().fillna(0).values\n",
        "  df['expanding_min'] = df.groupby(group)['signal'].expanding().min().fillna(0).values\n",
        "  df['expanding_range'] = df['expanding_max'] - df['expanding_min']\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abANAaO5T4l6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\n",
        "def run_feat_engineering(df, batch_size):\n",
        "    # create batches\n",
        "    df = batching(df, batch_size = batch_size)\n",
        "\n",
        "    # create leads and lags\n",
        "    df = lag_with_pct_change(df, np.asarray(range(1, 3), dtype=np.int32))\n",
        "\n",
        "    # create rolling stats\n",
        "    # df = calc_roll_stats(df, [3, 10, 50, 100, 500, 1000]) # groupごと(4000)\n",
        "    df = calc_roll_stats(df, [100, 1000])\n",
        "    # df = calc_roll_stats(df, [50000, 100000], group='batch') # batchごと(500000)\n",
        "\n",
        "    # create expanding stats\n",
        "    # df = calc_expand_stats(df)\n",
        "\n",
        "    # create signal ** 2 (this is the new feature)\n",
        "    df['signal_2'] = df['signal'] ** 2\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkDA5AB3TB13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fillna with the mean and select features for training\n",
        "def feature_selection(train, test):\n",
        "    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time', 'batch', 'train_group', 'test_group']]\n",
        "    train = train.replace([np.inf, -np.inf], np.nan)\n",
        "    test = test.replace([np.inf, -np.inf], np.nan)\n",
        "    for feature in features:\n",
        "        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n",
        "        train[feature] = train[feature].fillna(feature_mean)\n",
        "        test[feature] = test[feature].fillna(feature_mean)\n",
        "    return train, test, features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBDhfByvRp89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_class_weight(classes, exp=1):\n",
        "    '''\n",
        "    Weight of the class is inversely proportional to the population of the class.\n",
        "    There is an exponent for adding more weight.\n",
        "    '''\n",
        "    hist, _ = np.histogram(classes, bins=np.arange(12)-0.5)\n",
        "    class_weight = hist.sum()/np.power(hist, exp)\n",
        "    \n",
        "    return class_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arzvPi4DTJNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main function to perfrom groupkfold cross validation (we have 1000 vectores of 4000 rows and 8 features (columns)). Going to make 5 groups with this subgroups.\n",
        "def run_cv_model_by_batch(train, test, splits, batch_col, feats, sample_submission):\n",
        "    oof_ = np.zeros(len(train))\n",
        "    preds_ = np.zeros(len(test))\n",
        "    target = ['open_channels']\n",
        "    group = train['group']\n",
        "    # kf = GroupKFold(n_splits=5)\n",
        "    kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
        "\n",
        "    for n_fold, (tr_idx, val_idx) in enumerate(kf.split(train, train[target], groups=group)):\n",
        "        train_x, train_y = train.iloc[tr_idx], train[target].iloc[tr_idx]\n",
        "        valid_x, valid_y = train.iloc[val_idx], train[target].iloc[val_idx]\n",
        "        print(f'Our training dataset shape is {train_x.shape}')\n",
        "        print(f'Our validation dataset shape is {valid_x.shape}')\n",
        "\n",
        "        class_weight = get_class_weight(train_y)\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "        train_set = lgb.Dataset(train_x[feats], train_y)\n",
        "        val_set = lgb.Dataset(valid_x[feats], valid_y)\n",
        "\n",
        "        params = {'boosting_type': 'gbdt',\n",
        "                'metric': 'rmse',\n",
        "                'objective': 'regression',\n",
        "                'n_jobs': -1,\n",
        "                'seed': 236,\n",
        "                'num_leaves': 280,\n",
        "                'learning_rate': 0.026623466966581126,\n",
        "                'max_depth': 73,\n",
        "                'lambda_l1': 2.959759088169741,\n",
        "                'lambda_l2': 1.331172832164913,\n",
        "                'bagging_fraction': 0.9655406551472153,\n",
        "                'bagging_freq': 9,\n",
        "                'colsample_bytree': 0.6867118652742716}\n",
        "        \n",
        "        lgb_model = lgb.train(\n",
        "            params, \n",
        "            train_set, num_boost_round = 10000, \n",
        "            early_stopping_rounds = 50, \n",
        "            valid_sets = [train_set, val_set], \n",
        "            valid_names = ['train', 'eval'],\n",
        "            verbose_eval = 100,)\n",
        "\n",
        "        preds_f = lgb_model.predict(valid_x[feats])\n",
        "        f1_score_ = f1_score(valid_y,  np.round(np.clip(preds_f, 0, 10)).astype(int), average = 'macro')\n",
        "        print(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n",
        "        oof_[val_idx] += preds_f\n",
        "        te_preds = lgb_model.predict(test[feats])\n",
        "        preds_ += te_preds / SPLITS\n",
        "\n",
        "    return preds_, oof_\n",
        "\n",
        "    # f1_score_ = f1_score(train[target],  np.round(np.clip(oof_, 0, 10)).astype(int), average = 'macro')\n",
        "    # print(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n",
        "    # sample_submission['open_channels'] = np.round(np.clip(preds_, 0, 10)).astype(int)\n",
        "    # sample_submission.to_csv('lgb_submission.csv', index=False, float_format='%.4f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOM_WZ_5ZiYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        if col!='open_channels':\n",
        "            col_type = df[col].dtypes\n",
        "            if col_type in numerics:\n",
        "                c_min = df[col].min()\n",
        "                c_max = df[col].max()\n",
        "                if str(col_type)[:3] == 'int':\n",
        "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                        df[col] = df[col].astype(np.int8)\n",
        "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                        df[col] = df[col].astype(np.int16)\n",
        "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                        df[col] = df[col].astype(np.int64)  \n",
        "                else:\n",
        "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                        df[col] = df[col].astype(np.float16)\n",
        "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    else:\n",
        "                        df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esUCbE6mTLxE",
        "colab_type": "code",
        "outputId": "2b579a19-9497-498d-c530-5a2db553d9f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Reading Data Started...')\n",
        "train, test, sample_submission = read_data()\n",
        "train, test = batching_10(train, test)\n",
        "train = create_signal_mod(train)\n",
        "\n",
        "# # Kalman Filter\n",
        "# observation_covariance = .0015\n",
        "# train['signal'] = Kalman1D(train.signal.values,observation_covariance)\n",
        "# test['signal'] = Kalman1D(test.signal.values,observation_covariance)\n",
        "\n",
        "train, test = normalize(train, test)\n",
        "print('Reading and Normalizing Data Completed')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading Data Started...\n",
            "Reading and Normalizing Data Completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYu0glRGTPu2",
        "colab_type": "code",
        "outputId": "5c9cc623-3b3c-45a6-dae4-a133ae8645e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print('Creating Features')\n",
        "print('Feature Engineering Started...')\n",
        "\n",
        "train, test = add_category(train, test)\n",
        "\n",
        "# train, test = batching_10(train, test)\n",
        "\n",
        "train = run_feat_engineering(train, batch_size = GROUP_BATCH_SIZE)\n",
        "test = run_feat_engineering(test, batch_size = GROUP_BATCH_SIZE)\n",
        "\n",
        "print('Reduce memory usage...')\n",
        "train = reduce_mem_usage(train)\n",
        "test = reduce_mem_usage(test)\n",
        "\n",
        "tr_clean = pd.read_csv('./train_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n",
        "ts_clean = pd.read_csv('./test_clean.csv', dtype={'time': np.float32, 'signal': np.float32})\n",
        "pre_train = divide_and_add_features(tr_clean.signal, signal_size=500000)\n",
        "pre_test = divide_and_add_features(ts_clean.signal, signal_size=500000)\n",
        "pre_train.drop('signal', axis=1, inplace=True)\n",
        "pre_test.drop('signal', axis=1, inplace=True)\n",
        "pre_train.reset_index(inplace = True, drop = True)\n",
        "pre_test.reset_index(inplace = True, drop = True)\n",
        "train = pd.concat([train, pre_train], axis=1)\n",
        "test = pd.concat([test, pre_test], axis=1)\n",
        "\n",
        "del pre_train, pre_test, tr_clean, ts_clean\n",
        "gc.collect\n",
        "\n",
        "print('Reduce memory usage...')\n",
        "train = reduce_mem_usage(train)\n",
        "test = reduce_mem_usage(test)\n",
        "\n",
        "train, test, features = feature_selection(train, test)\n",
        "print('Feature Engineering Completed...')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating Features\n",
            "Feature Engineering Started...\n",
            "Reduce memory usage...\n",
            "Mem. usage decreased to 622.53 Mb (64.5% reduction)\n",
            "Mem. usage decreased to 265.01 Mb (63.0% reduction)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:09<00:00,  1.05it/s]\n",
            "100%|██████████| 4/4 [00:03<00:00,  1.05it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Reduce memory usage...\n",
            "Mem. usage decreased to 1137.52 Mb (57.6% reduction)\n",
            "Mem. usage decreased to 471.01 Mb (56.7% reduction)\n",
            "Feature Engineering Completed...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h448uvloaCSb",
        "colab_type": "code",
        "outputId": "d73b251e-dceb-47d7-b304-7d4974672704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('Reduce memory usage...')\n",
        "train = reduce_mem_usage(train)\n",
        "test = reduce_mem_usage(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reduce memory usage...\n",
            "Mem. usage decreased to 1137.52 Mb (0.0% reduction)\n",
            "Mem. usage decreased to 471.01 Mb (0.0% reduction)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGptgjoR9m_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_grouping(train, test):\n",
        "  train.loc[0:1000000, 'train_group'] = 0          # batch 0 and 1\n",
        "  train.loc[1000000:1500000, 'train_group'] = 1  # batch 2\n",
        "  train.loc[1500000:2000000, 'train_group'] = 2  # batch 3\n",
        "  train.loc[2000000:2500000, 'train_group'] = 3  # batch 4\n",
        "  train.loc[2500000:3000000, 'train_group'] = 4  # batch 5\n",
        "  train.loc[3000000:3500000, 'train_group'] = 1  # batch 6\n",
        "  train.loc[3500000:4000000, 'train_group'] = 2  # batch 7\n",
        "  train.loc[4000000:4500000, 'train_group'] = 4  # batch 8\n",
        "  train.loc[4500000:5000001, 'train_group'] = 3  # batch 9\n",
        "\n",
        "  test.loc[0:100000, 'test_group'] = 0\n",
        "  test.loc[100000:200000, 'test_group'] = 2\n",
        "  test.loc[200000:300000, 'test_group'] = 4\n",
        "  test.loc[300000:400000, 'test_group'] = 0\n",
        "  test.loc[400000:500000, 'test_group'] = 1\n",
        "  test.loc[500000:600000, 'test_group'] = 3\n",
        "  test.loc[600000:700000, 'test_group'] = 4\n",
        "  test.loc[700000:800000, 'test_group'] = 3\n",
        "  test.loc[800000:900000, 'test_group'] = 0\n",
        "  test.loc[900000:1000000, 'test_group'] = 2\n",
        "  test.loc[1000000:, 'test_group'] = 0\n",
        "\n",
        "  return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEzL3Kv5Tviy",
        "colab_type": "code",
        "outputId": "ab73a91d-535d-42ab-ea15-12c747be2201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# print(f'Training lgb model with {SPLITS} folds of GroupKFold Started...')\n",
        "# run_cv_model_by_batch(train, test, SPLITS, 'group', features, sample_submission)\n",
        "# print('Training completed...')\n",
        "\n",
        "train. test = train_grouping(train, test)\n",
        "preds = []\n",
        "oofs = []\n",
        "print('Training started')\n",
        "for i in range(5):\n",
        "  print(f'Training train_group {i+1} started...')\n",
        "  group_train = train[train['train_group'] == i]\n",
        "  group_test = test[test['test_group'] == i]\n",
        "  pred, oof = run_cv_model_by_batch(group_train, group_test, SPLITS, 'group', features, sample_submission)\n",
        "  # preds += pred.tolist()\n",
        "  # oofs += oof.tolist()\n",
        "  preds.append(pred)\n",
        "  oofs.append(oof)\n",
        "  print(f'Training train_group {i+1} completed...')\n",
        "print('Training completed...')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training started\n",
            "Training train_group 1 started...\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.0193377\teval's rmse: 0.0209298\n",
            "[200]\ttrain's rmse: 0.0130825\teval's rmse: 0.0167477\n",
            "[300]\ttrain's rmse: 0.0112324\teval's rmse: 0.0167239\n",
            "Early stopping, best iteration is:\n",
            "[297]\ttrain's rmse: 0.0112833\teval's rmse: 0.0167151\n",
            "Training fold 1 completed. macro f1 score : 0.99720\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.0194462\teval's rmse: 0.0205321\n",
            "[200]\ttrain's rmse: 0.0132764\teval's rmse: 0.0164039\n",
            "Early stopping, best iteration is:\n",
            "[204]\ttrain's rmse: 0.0131965\teval's rmse: 0.0163964\n",
            "Training fold 2 completed. macro f1 score : 0.99724\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.0194487\teval's rmse: 0.0204559\n",
            "[200]\ttrain's rmse: 0.0132228\teval's rmse: 0.0163018\n",
            "Early stopping, best iteration is:\n",
            "[189]\ttrain's rmse: 0.0134946\teval's rmse: 0.0162961\n",
            "Training fold 3 completed. macro f1 score : 0.99761\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.0192154\teval's rmse: 0.0214053\n",
            "[200]\ttrain's rmse: 0.0128954\teval's rmse: 0.0173481\n",
            "Early stopping, best iteration is:\n",
            "[220]\ttrain's rmse: 0.0125056\teval's rmse: 0.0173165\n",
            "Training fold 4 completed. macro f1 score : 0.99717\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.0192854\teval's rmse: 0.0205717\n",
            "[200]\ttrain's rmse: 0.0132116\teval's rmse: 0.0164932\n",
            "Early stopping, best iteration is:\n",
            "[188]\ttrain's rmse: 0.01348\teval's rmse: 0.016474\n",
            "Training fold 5 completed. macro f1 score : 0.99754\n",
            "Training train_group 1 completed...\n",
            "Training train_group 2 started...\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.0482029\teval's rmse: 0.0510464\n",
            "[200]\ttrain's rmse: 0.0343839\teval's rmse: 0.0421603\n",
            "Early stopping, best iteration is:\n",
            "[204]\ttrain's rmse: 0.0341278\teval's rmse: 0.0421403\n",
            "Training fold 1 completed. macro f1 score : 0.99680\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.0478863\teval's rmse: 0.0512031\n",
            "[200]\ttrain's rmse: 0.0340925\teval's rmse: 0.0422508\n",
            "Early stopping, best iteration is:\n",
            "[244]\ttrain's rmse: 0.0315211\teval's rmse: 0.042216\n",
            "Training fold 2 completed. macro f1 score : 0.99701\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.0480786\teval's rmse: 0.0504263\n",
            "[200]\ttrain's rmse: 0.0348334\teval's rmse: 0.0412006\n",
            "Early stopping, best iteration is:\n",
            "[205]\ttrain's rmse: 0.0345454\teval's rmse: 0.0411943\n",
            "Training fold 3 completed. macro f1 score : 0.99699\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.0481013\teval's rmse: 0.0500412\n",
            "[200]\ttrain's rmse: 0.0345775\teval's rmse: 0.0409076\n",
            "Early stopping, best iteration is:\n",
            "[207]\ttrain's rmse: 0.0341409\teval's rmse: 0.0408954\n",
            "Training fold 4 completed. macro f1 score : 0.99711\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.0478434\teval's rmse: 0.0515798\n",
            "[200]\ttrain's rmse: 0.0341273\teval's rmse: 0.0428372\n",
            "Early stopping, best iteration is:\n",
            "[208]\ttrain's rmse: 0.0336529\teval's rmse: 0.0428164\n",
            "Training fold 5 completed. macro f1 score : 0.99685\n",
            "Training train_group 2 completed...\n",
            "Training train_group 3 started...\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.109442\teval's rmse: 0.116065\n",
            "[200]\ttrain's rmse: 0.0890479\teval's rmse: 0.102952\n",
            "[300]\ttrain's rmse: 0.0844485\teval's rmse: 0.102756\n",
            "[400]\ttrain's rmse: 0.0810393\teval's rmse: 0.102738\n",
            "Early stopping, best iteration is:\n",
            "[408]\ttrain's rmse: 0.0807924\teval's rmse: 0.102735\n",
            "Training fold 1 completed. macro f1 score : 0.98341\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.109386\teval's rmse: 0.116113\n",
            "[200]\ttrain's rmse: 0.0890457\teval's rmse: 0.102911\n",
            "[300]\ttrain's rmse: 0.0844342\teval's rmse: 0.102749\n",
            "Early stopping, best iteration is:\n",
            "[327]\ttrain's rmse: 0.0834444\teval's rmse: 0.102723\n",
            "Training fold 2 completed. macro f1 score : 0.98334\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.109481\teval's rmse: 0.115923\n",
            "[200]\ttrain's rmse: 0.0892545\teval's rmse: 0.10288\n",
            "[300]\ttrain's rmse: 0.0845193\teval's rmse: 0.102682\n",
            "Early stopping, best iteration is:\n",
            "[343]\ttrain's rmse: 0.0829949\teval's rmse: 0.102642\n",
            "Training fold 3 completed. macro f1 score : 0.98348\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.109261\teval's rmse: 0.116697\n",
            "[200]\ttrain's rmse: 0.0888964\teval's rmse: 0.10367\n",
            "[300]\ttrain's rmse: 0.0843252\teval's rmse: 0.103413\n",
            "[400]\ttrain's rmse: 0.080965\teval's rmse: 0.103345\n",
            "Early stopping, best iteration is:\n",
            "[412]\ttrain's rmse: 0.0805826\teval's rmse: 0.103336\n",
            "Training fold 4 completed. macro f1 score : 0.98326\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.109675\teval's rmse: 0.115494\n",
            "[200]\ttrain's rmse: 0.089277\teval's rmse: 0.10228\n",
            "[300]\ttrain's rmse: 0.084642\teval's rmse: 0.10207\n",
            "Early stopping, best iteration is:\n",
            "[347]\ttrain's rmse: 0.0829021\teval's rmse: 0.102041\n",
            "Training fold 5 completed. macro f1 score : 0.98362\n",
            "Training train_group 3 completed...\n",
            "Training train_group 4 started...\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.298119\teval's rmse: 0.302026\n",
            "[200]\ttrain's rmse: 0.277836\teval's rmse: 0.2863\n",
            "[300]\ttrain's rmse: 0.274453\teval's rmse: 0.286066\n",
            "[400]\ttrain's rmse: 0.271795\teval's rmse: 0.285942\n",
            "[500]\ttrain's rmse: 0.269352\teval's rmse: 0.285882\n",
            "[600]\ttrain's rmse: 0.26688\teval's rmse: 0.285738\n",
            "[700]\ttrain's rmse: 0.264517\teval's rmse: 0.28569\n",
            "[800]\ttrain's rmse: 0.2622\teval's rmse: 0.285653\n",
            "[900]\ttrain's rmse: 0.26003\teval's rmse: 0.285638\n",
            "[1000]\ttrain's rmse: 0.257763\teval's rmse: 0.285596\n",
            "Early stopping, best iteration is:\n",
            "[990]\ttrain's rmse: 0.258007\teval's rmse: 0.285592\n",
            "Training fold 1 completed. macro f1 score : 0.87379\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.297878\teval's rmse: 0.30295\n",
            "[200]\ttrain's rmse: 0.277595\teval's rmse: 0.287371\n",
            "[300]\ttrain's rmse: 0.274205\teval's rmse: 0.287163\n",
            "[400]\ttrain's rmse: 0.271583\teval's rmse: 0.28706\n",
            "[500]\ttrain's rmse: 0.269124\teval's rmse: 0.28699\n",
            "[600]\ttrain's rmse: 0.266784\teval's rmse: 0.286938\n",
            "[700]\ttrain's rmse: 0.264517\teval's rmse: 0.28688\n",
            "Early stopping, best iteration is:\n",
            "[746]\ttrain's rmse: 0.263498\teval's rmse: 0.286853\n",
            "Training fold 2 completed. macro f1 score : 0.87102\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.297721\teval's rmse: 0.303587\n",
            "[200]\ttrain's rmse: 0.277378\teval's rmse: 0.288175\n",
            "[300]\ttrain's rmse: 0.27397\teval's rmse: 0.287989\n",
            "[400]\ttrain's rmse: 0.271306\teval's rmse: 0.287891\n",
            "[500]\ttrain's rmse: 0.26882\teval's rmse: 0.287783\n",
            "Early stopping, best iteration is:\n",
            "[510]\ttrain's rmse: 0.268569\teval's rmse: 0.287769\n",
            "Training fold 3 completed. macro f1 score : 0.87690\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.29819\teval's rmse: 0.301542\n",
            "[200]\ttrain's rmse: 0.277902\teval's rmse: 0.286124\n",
            "[300]\ttrain's rmse: 0.274511\teval's rmse: 0.285903\n",
            "[400]\ttrain's rmse: 0.271787\teval's rmse: 0.285764\n",
            "[500]\ttrain's rmse: 0.26937\teval's rmse: 0.285665\n",
            "[600]\ttrain's rmse: 0.267013\teval's rmse: 0.285614\n",
            "[700]\ttrain's rmse: 0.264762\teval's rmse: 0.28558\n",
            "Early stopping, best iteration is:\n",
            "[697]\ttrain's rmse: 0.264831\teval's rmse: 0.285579\n",
            "Training fold 4 completed. macro f1 score : 0.78387\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.298171\teval's rmse: 0.30196\n",
            "[200]\ttrain's rmse: 0.277818\teval's rmse: 0.286077\n",
            "[300]\ttrain's rmse: 0.274424\teval's rmse: 0.285864\n",
            "[400]\ttrain's rmse: 0.271772\teval's rmse: 0.285716\n",
            "[500]\ttrain's rmse: 0.269298\teval's rmse: 0.285667\n",
            "[600]\ttrain's rmse: 0.266919\teval's rmse: 0.285635\n",
            "[700]\ttrain's rmse: 0.264565\teval's rmse: 0.28556\n",
            "[800]\ttrain's rmse: 0.262202\teval's rmse: 0.285516\n",
            "[900]\ttrain's rmse: 0.260009\teval's rmse: 0.285489\n",
            "[1000]\ttrain's rmse: 0.257809\teval's rmse: 0.285468\n",
            "[1100]\ttrain's rmse: 0.255702\teval's rmse: 0.285439\n",
            "[1200]\ttrain's rmse: 0.253638\teval's rmse: 0.285451\n",
            "Early stopping, best iteration is:\n",
            "[1160]\ttrain's rmse: 0.254488\teval's rmse: 0.285428\n",
            "Training fold 5 completed. macro f1 score : 0.79186\n",
            "Training train_group 4 completed...\n",
            "Training train_group 5 started...\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.14116\teval's rmse: 0.146215\n",
            "[200]\ttrain's rmse: 0.117702\teval's rmse: 0.129271\n",
            "[300]\ttrain's rmse: 0.113612\teval's rmse: 0.129083\n",
            "Early stopping, best iteration is:\n",
            "[329]\ttrain's rmse: 0.112642\teval's rmse: 0.129065\n",
            "Training fold 1 completed. macro f1 score : 0.97460\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.141275\teval's rmse: 0.145362\n",
            "[200]\ttrain's rmse: 0.117886\teval's rmse: 0.128483\n",
            "[300]\ttrain's rmse: 0.113685\teval's rmse: 0.128362\n",
            "Early stopping, best iteration is:\n",
            "[323]\ttrain's rmse: 0.11288\teval's rmse: 0.128353\n",
            "Training fold 2 completed. macro f1 score : 0.97305\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.140919\teval's rmse: 0.146912\n",
            "[200]\ttrain's rmse: 0.117423\teval's rmse: 0.13005\n",
            "[300]\ttrain's rmse: 0.113205\teval's rmse: 0.129947\n",
            "Early stopping, best iteration is:\n",
            "[288]\ttrain's rmse: 0.113638\teval's rmse: 0.129939\n",
            "Training fold 3 completed. macro f1 score : 0.96930\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.140922\teval's rmse: 0.146612\n",
            "[200]\ttrain's rmse: 0.117336\teval's rmse: 0.130063\n",
            "[300]\ttrain's rmse: 0.113161\teval's rmse: 0.129925\n",
            "[400]\ttrain's rmse: 0.110061\teval's rmse: 0.129891\n",
            "Early stopping, best iteration is:\n",
            "[429]\ttrain's rmse: 0.109212\teval's rmse: 0.129882\n",
            "Training fold 4 completed. macro f1 score : 0.97244\n",
            "Our training dataset shape is (800000, 97)\n",
            "Our validation dataset shape is (200000, 97)\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttrain's rmse: 0.140879\teval's rmse: 0.146877\n",
            "[200]\ttrain's rmse: 0.117446\teval's rmse: 0.130223\n",
            "[300]\ttrain's rmse: 0.113232\teval's rmse: 0.130075\n",
            "[400]\ttrain's rmse: 0.110097\teval's rmse: 0.130016\n",
            "[500]\ttrain's rmse: 0.107319\teval's rmse: 0.13\n",
            "[600]\ttrain's rmse: 0.104796\teval's rmse: 0.129995\n",
            "[700]\ttrain's rmse: 0.102327\teval's rmse: 0.129986\n",
            "Early stopping, best iteration is:\n",
            "[684]\ttrain's rmse: 0.102692\teval's rmse: 0.129976\n",
            "Training fold 5 completed. macro f1 score : 0.97175\n",
            "Training train_group 5 completed...\n",
            "Training completed...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCog4u03y50S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_oof_data(arr):\n",
        "  arr_ = []\n",
        "  arr_ += arr[0][0:].tolist()\n",
        "  arr_ += arr[1][0:500000].tolist()\n",
        "  arr_ += arr[2][0:500000].tolist()\n",
        "  arr_ += arr[3][0:500000].tolist()\n",
        "  arr_ += arr[4][0:500000].tolist()\n",
        "  arr_ += arr[1][500000:].tolist()\n",
        "  arr_ += arr[2][500000:].tolist()\n",
        "  arr_ += arr[4][500000:].tolist()\n",
        "  arr_ += arr[3][500000:].tolist()\n",
        "  return arr_\n",
        "\n",
        "full_oof = sort_oof_data(oofs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg8cGTBpy76J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_pred_data(arr):\n",
        "  arr_ = []\n",
        "  arr_ += arr[0][0:100000].tolist()\n",
        "  arr_ += arr[2][0:100000].tolist()\n",
        "  arr_ += arr[4][0:100000].tolist()\n",
        "  arr_ += arr[0][100000:200000].tolist()\n",
        "  arr_ += arr[1][0:100000].tolist()\n",
        "  arr_ += arr[3][0:100000].tolist()\n",
        "  arr_ += arr[4][100000:].tolist()\n",
        "  arr_ += arr[3][100000:].tolist()\n",
        "  arr_ += arr[0][200000:300000].tolist()\n",
        "  arr_ += arr[2][100000:].tolist()\n",
        "  arr_ += arr[0][300000:].tolist()\n",
        "  return arr_\n",
        "\n",
        "full_pred = sort_pred_data(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8LawFfpy9uw",
        "colab_type": "code",
        "outputId": "c60dd432-a170-4e87-cc44-edd565090902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f1_score_ = f1_score(train.open_channels,  np.round(np.clip(full_oof, 0, 10)).astype(int), average = 'macro')\n",
        "print(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n",
        "sample_submission['open_channels'] = np.round(np.clip(full_pred, 0, 10)).astype(int)\n",
        "sample_submission.to_csv('lgb_submission.csv', index=False, float_format='%.4f')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training completed. oof macro f1 score : 0.93982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8N2QuObT6Rf",
        "colab_type": "code",
        "outputId": "7b721807-9010-4f23-a41d-565decc2b995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle competitions submit -f './lgb_submission.csv' -m 'lgb' liverpool-ion-switching"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 21.0M/21.0M [00:00<00:00, 42.5MB/s]\n",
            "Successfully submitted to University of Liverpool - Ion Switching"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwmBMaXEmdGW",
        "colab_type": "text"
      },
      "source": [
        "training with clean_kalman\n",
        "> Training completed. oof macro f1 score : 0.93940  \n",
        "> LB : 0.940  \n",
        "\n",
        "clean_kalman  \n",
        "add category  \n",
        "add 20shifted\n",
        "> Training completed. oof macro f1 score : 0.93941  \n",
        "> LB : 0.940\n",
        "\n",
        "clean_kalman  \n",
        "add category  \n",
        "add 20shifted  \n",
        "add target encoding(group)\n",
        "> Training completed. oof macro f1 score : 0.93880  \n",
        "> LB : 0.653\n",
        "\n",
        "clean_kalman + add category + add 20shifted\n",
        "> Training completed. oof macro f1 score : 0.93953  \n",
        "> LB : 0.942  \n",
        "\n",
        "clean_kalman + add category + add 20shifted + target encording(category)  \n",
        ">　Training completed. oof macro f1 score : 0.93842  \n",
        ">　LB : 0.941"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caPUEw17moRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}